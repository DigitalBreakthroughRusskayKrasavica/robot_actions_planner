{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MmmJSLbW3fkQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqJCn9wAbiUg"
      },
      "source": [
        "# Sorter by Russkaya Krasavica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-1fB5PtblFz"
      },
      "source": [
        "## Установка библиотек и импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "suIGzxRdbTB1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Mi_HWkbTB3"
      },
      "source": [
        "## Датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "j9ssuP4gbTB4"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class Step:\n",
        "    action: str = \"\"\n",
        "    text: str = \"\"\n",
        "    arguments: List[str] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class SorterTask():\n",
        "    action: str = \"\"\n",
        "    text: str = \"\"\n",
        "    goal: str = \"\"\n",
        "    task_type: int = -1\n",
        "    plan_id: int = -1\n",
        "    image: str = \"\"\n",
        "    steps: List[Step] = field(default_factory=list)\n",
        "    arguments: List[str] = field(default_factory=list)\n",
        "\n",
        "    def to_list(self):\n",
        "        return [[step.action, [arg for arg in step.arguments]] for step in self.steps]\n",
        "\n",
        "class SorterDataset(Dataset):\n",
        "    def __init__(self, path_to_csv: str = \"\"):\n",
        "        with open(path_to_csv, 'r') as f:\n",
        "            self._data = json.load(f)\n",
        "        self._size = len(self._data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._size\n",
        "\n",
        "    def __getitem__(self, idx) -> SorterTask:\n",
        "        entry = self._data[idx]\n",
        "        steps = []\n",
        "        return SorterTask(goal=entry['goal_eng'],\n",
        "                        steps=steps,\n",
        "                        task_type=entry['task_type'],\n",
        "                        plan_id=entry[\"plan_id\"], image=entry['image'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to3b-D3SbTB7"
      },
      "source": [
        "## Генерация плана"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "OE_5XW0UbTB7"
      },
      "outputs": [],
      "source": [
        "class FullPlanGeneration():\n",
        "    def __init__(self, model):\n",
        "        self._model = model\n",
        "\n",
        "    def predict(self, gt_task: SorterTask) -> SorterTask:\n",
        "        steps = self._model.generate(gt_task)\n",
        "        gt_task.steps = steps\n",
        "        return gt_task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh9-9F7WbTB8",
        "outputId": "7391c645-6b71-4463-f5ab-0274f2d8f77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SorterTask(action='', text='', goal='Put the gray cat in an orange container.', task_type=0, plan_id=0, image='2176356811276008251_0.png', steps=[], arguments=[])\n"
          ]
        }
      ],
      "source": [
        "path_to_csv = \"/content/test_dataset.json\"\n",
        "dataset = SorterDataset(path_to_csv=path_to_csv)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import suppress\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens.token import Token\n",
        "\n",
        "class NLPModel:\n",
        "    def __init__(self):\n",
        "        self.model = spacy.load('en_core_web_sm')\n",
        "\n",
        "    # Рекурсивная функция получения всех дочерних токенов\n",
        "    def insert_all_children(self, token, result: list, first_it: bool = False) -> list[None] | None:\n",
        "        if not first_it and token.pos_ != 'DET':\n",
        "            result.insert(0, token.text)\n",
        "        if list(token.children):\n",
        "            return [self.insert_all_children(child, result) for child in reversed(list(token.children))]\n",
        "\n",
        "\n",
        "    def get_full_prep(self, prep: Token) -> str:\n",
        "        temp = prep\n",
        "        full_prep = prep.text\n",
        "        while hasattr(temp, 'head') and temp.head.pos_ != \"VERB\":\n",
        "            full_prep = temp.head.text + ' ' + full_prep\n",
        "            temp = temp.head\n",
        "        return full_prep\n",
        "\n",
        "    def find_nouns_and_preps(self, tokens: list[Token], text: str) -> tuple[dict[str, str], list[str]]:\n",
        "        preps = {} # словарь предлогов + связанных с ними словосочетаний\n",
        "        nouns = [] # список всех словосочетаний в предложении\n",
        "\n",
        "        existing_nouns = []\n",
        "        # Собираем имена существительные и имена собственные\n",
        "        for noun in sorted(filter(lambda token: token.pos_ in ('NOUN', 'PROPN'), tokens),\n",
        "                        key=lambda x: -len(list(x.children))):\n",
        "            subj = [noun.text]\n",
        "            self.insert_all_children(noun, subj, first_it=True)\n",
        "\n",
        "            # если имя сущ. уже было добавлено, как зависимость от другого, то оно существует\n",
        "            if not any(set(ex_subj) | set(subj) == set(ex_subj) for ex_subj in existing_nouns):\n",
        "                nouns.append(subj)\n",
        "            existing_nouns.append(subj)\n",
        "\n",
        "        # Собираем предлоги и их зависимости\n",
        "        for prep in filter(lambda token: token.pos_ == \"ADP\", tokens):\n",
        "            deps = []\n",
        "            self.insert_all_children(prep, deps, first_it=True)\n",
        "\n",
        "            if prep.dep_ == 'prt':\n",
        "                continue\n",
        "\n",
        "            prep = self.get_full_prep(prep)\n",
        "            preps[prep] = ' '.join(deps)\n",
        "\n",
        "        # сортировка по порядку слов в предложении\n",
        "        nouns = sorted(nouns, key=lambda noun: text.index(noun[0]))\n",
        "        return preps, [' '.join(noun) for noun in nouns]\n",
        "\n",
        "\n",
        "    # Оставляем только существительные с предлогами и прилагательными, которые к ней относятся\n",
        "    def generate(self, string: str) -> list[str | Any]:\n",
        "        tokenized = self.model(string)\n",
        "        prepositions, nouns = self.find_nouns_and_preps(tokenized, string)\n",
        "\n",
        "        # содержит три главных словосочетания, которые будут ипользоваться в модели компьютерного зрения\n",
        "        result = ['', '', '']\n",
        "\n",
        "        nouns = nouns + ['' for _ in range(3 - len(nouns))]\n",
        "        for preposition, noun in prepositions.items():\n",
        "            if preposition.lower() in ('from', 'off'):\n",
        "                result[1] = noun\n",
        "            else:\n",
        "                result[2] = noun\n",
        "            # Ignore possible ValueError\n",
        "            with suppress(ValueError):\n",
        "                nouns.remove(noun)\n",
        "\n",
        "        result[0] = nouns[0]\n",
        "        return result"
      ],
      "metadata": {
        "id": "uKd8bVdrIsKe"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# class NLPModel:\n",
        "#   def generate(self, task):\n",
        "#     tokenized = nltk.word_tokenize(task)\n",
        "#     nouns = [\"\", \"\", \"\"]\n",
        "\n",
        "#     result = \"\"\n",
        "\n",
        "#     identify = 0\n",
        "#     for (word, pos) in nltk.pos_tag(tokenized):\n",
        "#         if pos in [\"IN\", \"TO\"] and word in [\"from\", \"off\"]:\n",
        "#             nouns[identify] = result[:-1]\n",
        "#             result = \"\"\n",
        "#             identify = 1\n",
        "#         elif pos in [\"IN\", \"TO\"]:\n",
        "#             nouns[identify] = result[:-1]\n",
        "#             result = \"\"\n",
        "#             identify = 2\n",
        "#         result = result + word + ' ' if pos in ['NN', 'JJ'] else result\n",
        "#     nouns[identify] = result[:-1]\n",
        "#     return nouns\n"
      ],
      "metadata": {
        "id": "iVMzGrW__3vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979972d8-7b3d-4d22-9376-7ff0ce75d40e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)\n",
        "%cd ~\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd ~/GroundingDINO\n",
        "!pip install -q -e .\n",
        "import os\n",
        "CONFIG_PATH = \"~/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\"\n",
        "%cd\n",
        "!mkdir ~/weights\n",
        "%cd ~/weights\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth"
      ],
      "metadata": {
        "id": "IHASyQodGBx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6815e7e1-afc2-4d32-8e46-0fad434a81bc"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/root\n",
            "fatal: destination path 'GroundingDINO' already exists and is not an empty directory.\n",
            "/root/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/root\n",
            "mkdir: cannot create directory ‘/root/weights’: File exists\n",
            "/root/weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "WEIGHTS_NAME = \"groundingdino_swinb_cogcoor.pth\"\n",
        "WEIGHTS_PATH = '/root/weights/'+WEIGHTS_NAME\n",
        "#print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))"
      ],
      "metadata": {
        "id": "XAt9AgMQGIOL"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/GroundingDINO\n",
        "\n",
        "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
        "\n",
        "dino_model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
      ],
      "metadata": {
        "id": "ayiDbIsOGIxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "2f78a968-dd63-430f-f7a1-0532478adace"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/GroundingDINO\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-42c0609fc6b2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdino_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/GroundingDINO/groundingdino/util/inference.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_config_path, model_checkpoint_path, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSLConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/GroundingDINO/groundingdino/models/__init__.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODULE_BUILD_FUNCS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbuild_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODULE_BUILD_FUNCS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py\u001b[0m in \u001b[0;36mbuild_groundingdino\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0msub_sentence_present\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_sentence_present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     model = GroundingDINO(\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backbone, transformer, num_queries, aux_loss, iter_update, query_dim, num_feature_levels, nheads, two_stage_type, dec_pred_bbox_embed_share, two_stage_class_embed_share, two_stage_bbox_embed_share, num_patterns, dn_number, dn_box_noise_scale, dn_label_noise_ratio, dn_labelbook_size, text_encoder_type, sub_sentence_present, max_text_len)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenlizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tokenlizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenlizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pretrained_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/GroundingDINO/groundingdino/util/get_tokenlizer.py\u001b[0m in \u001b[0;36mget_pretrained_language_model\u001b[0;34m(text_encoder_type)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_pretrained_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext_encoder_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext_encoder_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"roberta-base\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_encoder_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3085\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    142\u001b[0m             self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n\u001b[1;32m    143\u001b[0m                                     requires_grad=not _freeze)\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_padding_idx_with_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "from groundingdino.util import box_ops\n",
        "\n",
        "BOX_TRESHOLD = 0.25\n",
        "TEXT_TRESHOLD = 0.1\n",
        "ROBOT_NAME = \"robotic manipulator\"\n",
        "\n",
        "def get_boxes(prompt, image_path):\n",
        "  image_source, image = load_image(image_path)\n",
        "  boxes, logits, phrases = predict(\n",
        "      model=dino_model,\n",
        "      image=image,\n",
        "      caption=prompt,\n",
        "      box_threshold=BOX_TRESHOLD,\n",
        "      text_threshold=TEXT_TRESHOLD\n",
        "  )\n",
        "  return boxes, logits, phrases\n",
        "\n",
        "def get_full_boxes(what, from_obj, where, img):\n",
        "  result = [[] for _ in range(3)]\n",
        "  for obj in [ROBOT_NAME, what, from_obj, where]:\n",
        "    data = get_boxes(obj, img)\n",
        "    if len(data[0]) == 0:\n",
        "      data = [[[0, 0, 0, 0]], [0], [obj]]\n",
        "    best_i = max(range(len(data[0])), key=lambda i: data[1][i])\n",
        "    # if obj == ROBOT_NAME:\n",
        "    #   data[0][0][2] /= 3\n",
        "    #   data[0][0][3] /= 2\n",
        "    #   data[0][0][0] -= data[0][0][2]*0.4\n",
        "    #   data[0][0][1] -= data[0][0][3]*0.5\n",
        "\n",
        "    for i in range(3):\n",
        "      elem = data[i][best_i]\n",
        "      result[i] += [elem.numpy().tolist() if type(elem) == torch.Tensor else elem]\n",
        "\n",
        "  if DEBUG:\n",
        "    img_src, _ = load_image(img)\n",
        "    annotated_frame = annotate(image_source=img_src, boxes=torch.tensor(result[0]), logits=result[1], phrases=result[2])\n",
        "    %matplotlib inline\n",
        "    sv.plot_image(annotated_frame, (16, 16))\n",
        "\n",
        "  return {result[2][i]: box_ops.box_cxcywh_to_xyxy(torch.tensor(result[0][i])) for i in range(len(result[0]))}"
      ],
      "metadata": {
        "id": "2BDONeLHGKLI"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groundingdino.util import box_ops\n",
        "from torchvision.ops.boxes import box_area\n",
        "import torch\n",
        "\n",
        "# def almost_equals(area1, area2):\n",
        "#   return abs(area1 - area2) < 0.05\n",
        "\n",
        "def calculate_overflow(robot, thing):\n",
        "    robot = torch.tensor([robot.numpy()])\n",
        "    thing = torch.tensor([thing.numpy()])\n",
        "    area1 = box_area(robot).item()\n",
        "    area2 = box_area(thing).item()\n",
        "    # if area2 > area1: # if object is behind robot\n",
        "    #   return 0\n",
        "\n",
        "    # calculating intersection\n",
        "    lt = torch.max(robot[:, None, :2], thing[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(robot[:, None, 2:], thing[:, 2:])  # [N,M,2]\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = (wh[:, :, 0] * wh[:, :, 1])[0].item()  # [N,M]\n",
        "\n",
        "    # # object is robot itself\n",
        "    # if inter > 0 and almost_equals(area1, area2) and almost_equals(area1, inter):\n",
        "    #   print('inter', inter)\n",
        "    #   print('area1', area1)\n",
        "    #   print('found robot denoted as some object')\n",
        "    #   return -1\n",
        "\n",
        "    return inter / (area2 + 1e-6)\n",
        "\n",
        "\n",
        "NEAR_THRESHOLD = 0.1\n",
        "def is_near(box1, box2):\n",
        "  return calculate_overflow(box1, box2) > NEAR_THRESHOLD\n"
      ],
      "metadata": {
        "id": "6JJwikTOGQLm"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "@dataclass\n",
        "class Step:\n",
        "    action: str = \"\"\n",
        "    text: str = \"\"\n",
        "    arguments: List[str] = field(default_factory=list)\n",
        "\n",
        "def get_commands(what, from_obj, where, img_path):\n",
        "  o = get_full_boxes(what, from_obj, where, img_path)\n",
        "  steps = []\n",
        "  if not is_near(o[ROBOT_NAME], o[what]):\n",
        "    steps.append(Step(action='move_to', arguments=[from_obj, what]))\n",
        "    steps.append(Step(action='pick_up', arguments=[from_obj, what]))\n",
        "  if where:\n",
        "    if not is_near(o[where], o[what]):\n",
        "      steps.append(Step(action='move_to', arguments=[where, what]))\n",
        "    if where:\n",
        "      steps.append(Step(action='put', arguments=[where, what]))\n",
        "  return steps\n",
        "\n",
        "def filter_empty(commands):\n",
        "  result = []\n",
        "  for cmd in commands:\n",
        "    if cmd.action == 'move_to' and not any(cmd.arguments) or cmd.action in ['pick_up', 'put'] and not cmd.arguments[1]:\n",
        "      continue\n",
        "    args = [a if a else 'unspecified' for a in cmd.arguments]\n",
        "    result.append(Step(action=cmd.action, text=cmd.text, arguments=args))\n",
        "\n",
        "  # for i in range(len(result)):\n",
        "  #   if result[i].action == 'move_to' and result[i].arguments[1] == 'unspecified' and len(result) == 1:\n",
        "  #     print('here')\n",
        "  #     result[i].arguments = result[i].arguments[::-1]\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "TxqawEztGSYc"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CVModel:\n",
        "  def generate(self, main_words, img_path):\n",
        "    print('got main words:', main_words)\n",
        "    return filter_empty(get_commands(*main_words, img_path))"
      ],
      "metadata": {
        "id": "Fk4MgY-AATfe"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PipelineModel:\n",
        "  def __init__(self, nlp_model, cv_model, img_prefix):\n",
        "    self.nlp_model = nlp_model\n",
        "    self.cv_model = cv_model\n",
        "\n",
        "  def generate(self, task):\n",
        "    return self.cv_model.generate(self.nlp_model.generate(task.goal), img_prefix + task.image)"
      ],
      "metadata": {
        "id": "0cUSW3RW-nPV"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "72orXfqmbTB9"
      },
      "outputs": [],
      "source": [
        "img_prefix = '/content/test_images/'\n",
        "pipeline = PipelineModel(NLPModel(), CVModel(), img_prefix)\n",
        "gen_method = FullPlanGeneration(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "BGSfnvzwbTB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f579f9-5c91-498d-8c2c-dd817aa9b7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "got main words: ['gray cat', '', 'orange container']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:905: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'plan_id': 0, 'image': '2176356811276008251_0.png', 'goal_eng': 'Put the gray cat in an orange container.', 'task_type': 0, 'plan': [['put', ['orange container', 'gray cat']]]}\n",
            "got main words: ['toy', '', 'floor']\n",
            "{'plan_id': 1, 'image': '2176356811276008251_0.png', 'goal_eng': 'Put the toy on the floor.', 'task_type': 0, 'plan': [['move_to', ['unspecified', 'toy']], ['pick_up', ['unspecified', 'toy']], ['move_to', ['floor', 'toy']], ['put', ['floor', 'toy']]]}\n",
            "got main words: ['garlic', 'table', 'chair']\n",
            "{'plan_id': 2, 'image': '2556860223905553708_0.png', 'goal_eng': 'Take the garlic from the table and put it on a chair.', 'task_type': 0, 'plan': [['put', ['chair', 'garlic']]]}\n",
            "got main words: ['apple', 'floor', '']\n",
            "{'plan_id': 3, 'image': '2556860223905553708_0.png', 'goal_eng': 'Pick up the apple from the floor.', 'task_type': 0, 'plan': [['move_to', ['floor', 'apple']], ['pick_up', ['floor', 'apple']]]}\n",
            "got main words: ['kitten', '', 'table']\n",
            "{'plan_id': 4, 'image': '2657671826402908683_0.png', 'goal_eng': 'Put the kitten on the table.', 'task_type': 0, 'plan': [['move_to', ['table', 'kitten']], ['put', ['table', 'kitten']]]}\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "DEBUG = False\n",
        "\n",
        "for i, task in list(enumerate(dataset)):\n",
        "    answer = {\n",
        "        'plan_id': task.plan_id,\n",
        "        'image': task.image,\n",
        "        'goal_eng': task.goal,\n",
        "        'task_type': task.task_type,\n",
        "      }\n",
        "\n",
        "    predicted_plan = gen_method.predict(task)\n",
        "    answer['plan'] = predicted_plan.to_list()\n",
        "\n",
        "    print(answer)\n",
        "    results.append(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "8p-sRnQobTB9"
      },
      "outputs": [],
      "source": [
        "with open('/content/results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "grWDM2XfbTB-"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "def calculate_metrics(path_to_test: str,\n",
        "                      path_to_results: str) -> float:\n",
        "    test_records = {}\n",
        "\n",
        "    with open(path_to_test, 'r') as f:\n",
        "        test_file = json.load(f)\n",
        "        for element in test_file:\n",
        "            test_records[element['plan_id']] = element['plan']\n",
        "\n",
        "    correct = {}\n",
        "\n",
        "\n",
        "    with open(path_to_results, 'r') as f:\n",
        "        results_file = json.load(f)\n",
        "        for element in results_file:\n",
        "            if test_records[element['plan_id']] == element['plan']:\n",
        "                correct[int(element['plan_id'])] = 1\n",
        "            else:\n",
        "              correct[int(element['plan_id'])] = 0\n",
        "\n",
        "    # print(len(test_records))\n",
        "    return correct, len(results_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "DRZKoOTKbTB-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "18285464-b5c4-440a-d505-2ff9ac39f489"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "correct, length = calculate_metrics(path_to_test='/content/train_dataset.json',\n",
        "                  path_to_results='/content/results.json')\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "x, y = [], []\n",
        "for plan_id, is_correct in correct.items():\n",
        "  x.append(plan_id)\n",
        "  y.append(is_correct)\n",
        "plt.bar(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(correct.values())/len(correct)"
      ],
      "metadata": {
        "id": "rUcrk9BhcYy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29cac319-82df-4bee-a25f-33fe90c805e8"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "Z-1fB5PtblFz",
        "l7Mi_HWkbTB3",
        "3ftpHKPebTB5",
        "OhfMT65nbTB5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}